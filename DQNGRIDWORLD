import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import deque


grid=np.zeros((5,5),dtype=int)
obstacles={(1,1),(1,2),(2,1),(2,2)}
start=[4,4]
goal=[0,0]
#print(grid)                                #4,4 represents the starting point of the grid
moves={'Up':[-1,0],'Down':[1,0],'Right':[0,1],'Left':[0,-1]} #Up, Down, Right, Left   #initialse the location array incase move chosen at start goes off grid
action_to_index = {k: i for i, k in enumerate(moves.keys())}
index_to_action = {i: k for k, i in action_to_index.items()}


class DQN(nn.Module):   #initialisng dqn for 2 (x,y) inputs and 4 outputs (actions)
    def __init__(self, state_size=2, action_size=4):   #neural network approximates without stoeing pairs
        super(DQN,self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128,128)
        self.fc3=nn.Linear(128,action_size)

    def forward(self,x):
        x=F.relu(self.fc1(x))
        x=F.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:  #stores previos run to train q value estimates
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return (torch.tensor(state, dtype=torch.float32),
                torch.tensor(action, dtype=torch.int64),
                torch.tensor(reward, dtype=torch.float32),
                torch.tensor(next_state, dtype=torch.float32),
                torch.tensor(done, dtype=torch.float32))
    def __len__(self):
        return len(self.buffer)

gamma = 0.98
epsilon = 0.1
epsilon_min=0.01
#decay=0.995
lr = 5e-4
batch_size = 128
target_update = 10
episodes = 50

policy_net = DQN()
target_net = DQN()
target_net.load_state_dict(policy_net.state_dict())
optimizer = optim.Adam(policy_net.parameters(), lr=lr)
buffer = ReplayBuffer()

def state_index(pos):
     state=np.array(pos,dtype=np.float32)  #locates the current state
     return state

def select_action(state):   #epsilon greedy principle
    if random.random() < epsilon:
        return np.random.randint(0,3)  # Random action index
    else:
        with torch.no_grad():
            return policy_net(torch.tensor(state).unsqueeze(0)).argmax().item()
#epsilon = max(epsilon_min, epsilon * decay)

def step(pos, action_index):
    action = moves[index_to_action[action_index]]
    newpos = [pos[0] + action[0], pos[1] + action[1]]

    if not (0 <= newpos[0] < 5 and 0 <= newpos[1] < 5):
        newpos = pos  # stay in place if out of bounds


    reward = -1
    if tuple(newpos) in obstacles:
        reward = -30
    elif newpos==pos:
        reward = -5
    elif newpos == goal:
        reward = 100

    done = (newpos == goal)
    return newpos, reward, done   #done=1 for epsiode end

for ep in range(episodes):  #play game
    pos = start.copy()
    state = np.array(pos, dtype=np.float32)
    total_reward = 0

    for t in range(100):
        action = select_action(state)
        next_pos, reward, done = step(pos, action)
        next_state = np.array(next_pos, dtype=np.float32)

        buffer.push(state, action, reward, next_state, done) #stores and sends to replay buffer
        state = next_state
        pos = next_pos
        total_reward += reward


        if len(buffer) >= batch_size:
            states, actions, rewards, next_states, dones = buffer.sample(batch_size)

            q_vals = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
            next_q_vals = target_net(next_states).max(1)[0]
            expected_q = rewards + gamma * next_q_vals * (1 - dones) #bellman eq for estimates

            loss = F.mse_loss(q_vals, expected_q.detach())
            optimizer.zero_grad()  #backpropgates loss
            loss.backward()  #computes gradients between calculated and expected
            optimizer.step()  #updates the network weights  #scipy minimise?

        if done:
            break

    if ep % target_update == 0:
        target_net.load_state_dict(policy_net.state_dict())

    print(f"Episode {ep}, Total Reward: {total_reward}")
