import numpy as np
import random
import matplotlib.pyplot as plt
from numpy import empty

grid=np.zeros((5,5),int)
grid[1:3,1:3]=1; grid[0,0]=10; grid[4,4]=7  #1s represent hole on the grid #7 represents agent path 10 represents end
#print(grid)                                #4,4 represents the starting point of the grid
moves={'Up':[-1,0],'Down':[1,0],'Right':[0,1],'Left':[0,-1]} #Up, Down, Right, Left   #initialse the location array incase move chosen at start goes off grid
beta=0.1
gamma = 0.9
epsilon=0.1

def state_index(pos):
    return pos[0] * 5 + pos[1] + 1   #locates the current state

#greedy epsilon policy
def action_choice(state, Q, epsilon):   #select a random choice if we are exploring the maze
    if random.random() < epsilon:
        return random.choice(list(moves.keys()))
    else:                                  #otherwise choose the currently best known path
        return max(Q[state], key=Q[state].get)   #it is important to describe the action array append as keys so they can be accesed in the epsilon grredy function like so

def move():   #defines the associated move for each state of the maze as well as its expected reward
    start = [4, 4]
    goal = [0, 0]
    #note keep seperate arrays for tracking the path history of the agent and its current position
    pos = start.copy()

    iters=0
    max_iters=100

    reward_array=[]
    state_array=[]
    action_array=[]

    while pos !=goal and iters<max_iters:
        current_state=state_index(pos)
        action_name=action_choice(current_state, Q, epsilon)  #now the move is no longer chosen randomly every time and wil follow epsilon greedy algo
        action= moves[action_name]  #action taken at each step now described in a vector

        newpos= [pos[0] + action[0], pos[1] + action[1]]

        if 0<=newpos[0]<5 and 0<=newpos[1]<5:   #defining bounds on the grid   #define allowed moves rather tha disallowed one
                pos=newpos                               #if out of bounds, take the last position/ stay there                                             #special case considered for start

        #ignores disallowed rules and continues loop
        if newpos==[1,1] or newpos==[1,2] or newpos==[2,1] or newpos==[2,2]:
            reward=-30   #reward system
        elif pos==goal:
            reward=100
        else:
            reward=-1


        reward_array.append(reward)
        state_array.append(current_state)
        action_array.append(action_name)

        iters +=1

    return state_array, action_array, reward_array   #action array is stored as a vector of keys as discussed in line 22




Q={}  #initilise Q dictionary
for state in range(1, 26):
    Q[state]={}
    for move_made in moves:   #dont update the Q table for all the possible moves taken in s state. only the one which was made
        Q[state][move_made]=0  #initialQ start as zeros

total_rewards = []
episode_steps=[]

for episode in range(1000):
    state_at, action_at, reward_at = move()
    total_rewards.append(sum(reward_at))
    episode_steps.append(len(action_at))

    for i in range(len(state_at)-1):  #qs updated based off where the agent is at  ###why does this allow me to get the next state?
        s = state_at[i]    #redefining the state posititons to be used in the qt calc
        s_next = state_at[i + 1]
        a=action_at[i]
        r = reward_at[i]

        # Q-learning update rule
        old_q = Q[s][a]     #use of 'a' means we are only updating the move acc chosen at s and not every possible move which could be made at s (which was how it was before)
        max_next_q = max(Q[s_next].values())
        new_q = old_q + beta * (r + gamma * max_next_q - old_q)  #this formula is similar to stochastic?
        Q[s][a] = new_q  #updates the old state as the now new state

        # Decay epsilon (less exploration over time)
    if episode % 100 == 0:
        epsilon = max(0.01, epsilon * 0.95)
        print(f"Episode {episode}, Avg Reward: {np.mean(total_rewards[-100:]):.2f}, Epsilon: {epsilon:.3f}")


# Analysis
        print("\n=== LEARNING ANALYSIS ===")
        print("Final Q-values (showing top 3 states):")
        for state in [21, 16, 11]:  # states near start
            print(f"State {state}: {Q[state]}")

        # Plot results
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 3, 1)
        plt.plot(total_rewards)
        plt.title('Total Rewards per Episode')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')

        plt.subplot(1, 3, 2)
        plt.plot(episode_steps)
        plt.title('Episode Length (Steps to Goal)')
        plt.xlabel('Episode')
        plt.ylabel('Steps')

        plt.subplot(1, 3, 3)
        # Show learned policy
        policy_grid = np.zeros((5, 5))
        arrow_map = {'Up': '↑', 'Down': '↓', 'Left': '←', 'Right': '→'}
        for state in range(1, 26):
            row = (state - 1) // 5
            col = (state - 1) % 5
            best_action = max(Q[state], key=Q[state].get)
            policy_grid[row, col] = list(moves.keys()).index(best_action)

        plt.imshow(policy_grid, cmap='viridis')
        plt.title('Learned Policy')
        plt.colorbar(label='Action: 0=Up, 1=Down, 2=Right, 3=Left')

        # Add grid lines and labels
        for i in range(5):
            for j in range(5):
                if [i, j] == [0, 0]:
                    plt.text(j, i, 'GOAL', ha='center', va='center', color='white', fontweight='bold')
                elif [i, j] == [4, 4]:
                    plt.text(j, i, 'START', ha='center', va='center', color='white', fontweight='bold')
                elif grid[i, j] == 1:
                    plt.text(j, i, 'HOLE', ha='center', va='center', color='red', fontweight='bold')

        plt.tight_layout()
        plt.savefig('qlearning_results.png', dpi=150, bbox_inches='tight')
        print("Plot saved as 'qlearning_results.png'")
        # plt.show()  # Comment out problematic show()

        print(f"\nFinal average reward (last 100 episodes): {np.mean(total_rewards[-100:]):.2f}")
        print(f"Final average episode length: {np.mean(episode_steps[-100:]):.2f}")
